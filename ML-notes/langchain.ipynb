{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRCU5iknK0HqasQU1FBgh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-Amrollahi/Personal-Notes/blob/master/ML-notes/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dq5z9F3GOHZb"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts"
      ],
      "metadata": {
        "id": "jHf349AWSWZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PFUnjq1PST0B",
        "outputId": "8e26a06d-1779-461b-bc29-83eaaef7581c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about chickens.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "messages = template.format_messages(\n",
        "    name=\"Bob\",\n",
        "    user_input=\"What is your name?\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pII4bv4sTEFX",
        "outputId": "c5e22539-7bcd-4ce8-f3b3-1f5c1e7cb400"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}),\n",
              " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, example=False),\n",
              " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, example=False),\n",
              " HumanMessage(content='What is your name?', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = HuggingFaceHub(repo_id = \"bigscience/bloom-3b\", huggingfacehub_api_token = \"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\")\n",
        "template = PromptTemplate(input_variables=[\"company\",\"type\"], template=\"{company} which is in field of {type}, is a company that \")\n",
        "chains = LLMChain(llm=llm,prompt=template)\n",
        "\n",
        "chains.run({\"company\":\"Google\",\"type\":\"Technology\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pl3muDPlON6V",
        "outputId": "c42be91d-e4ff-4901-d6ad-3d2e0c73faa6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' is a leader in the field of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to cache responses\n",
        "- We can also cahce them in sqlite"
      ],
      "metadata": {
        "id": "Q0Ir7GcdludV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.predict(\"Tell me a joke\")"
      ],
      "metadata": {
        "id": "rF5EhI2nPN7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}