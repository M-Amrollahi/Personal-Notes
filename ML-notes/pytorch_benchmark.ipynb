{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8XN4yrHzOwfAlZrUAk0/7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-Amrollahi/Personal-Notes/blob/master/ML-notes/pytorch_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yj6fRrigTKS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f_t1(x):\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(100,100),\n",
        "        nn.Linear(100,1000)\n",
        "    )\n",
        "\n",
        "    y = model.forward(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "t_QvIJDthHzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "x = torch.randn((30,100))\n",
        "\n",
        "t0 = timeit.Timer(\n",
        "    stmt='f_t1(x)',\n",
        "    setup='from __main__ import f_t1',\n",
        "    globals={'x': x})\n",
        "\n",
        "print(t0.timeit(100)/100*1000 , \"ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT7I-rW2he_j",
        "outputId": "9cc176c1-fed6-4304-ccbd-4cb59224cc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3574639199941885 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.benchmark as benchmark\n",
        "\n",
        "t0 = benchmark.Timer(\n",
        "    stmt='f_t1(x)',\n",
        "    setup='from __main__ import f_t1',\n",
        "    num_threads=10,\n",
        "    globals={'x': x})\n",
        "\n",
        "print(t0.timeit(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhLhdPPnh_Z7",
        "outputId": "c410eb59-1814-4c16-9a72-dcdcec950155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7a81230472b0>\n",
            "f_t1(x)\n",
            "setup: from __main__ import f_t1\n",
            "  3.29 ms\n",
            "  1 measurement, 100 runs , 10 threads\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the APIs are the same for the basic functionality, there are some important differences. benchmark.Timer.timeit() returns the time per run as opposed to the total runtime like timeit.Timer.timeit() does. PyTorch benchmark module also provides formatted string representations for printing the results.\n",
        "\n",
        "Another important difference, and the reason why the results diverge is that PyTorch benchmark module runs in a single thread by default. We can change the number of threads with the num_threads argument.\n",
        "\n",
        "https://pytorch.org/tutorials/recipes/recipes/benchmark.html"
      ],
      "metadata": {
        "id": "UaBRqHLvsUM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in the results between timeit and benchmark modules is because the timeit module is not synchronizing CUDA and is thus only timing the time to launch the kernel. PyTorchâ€™s benchmark module does the synchronization for us.\n",
        "\n"
      ],
      "metadata": {
        "id": "RJ_M8RZ136sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "timeit.Timer.autorange method:\n",
        "This method automatically determines the appropriate number of loops to run the timer for a sufficiently long time to get a reliable measurement. It adjusts the number of loops based on the time taken by the code being measured.\n",
        "\n",
        "```python\n",
        "import timeit\n",
        "\n",
        "# Define a simple code snippet to measure\n",
        "code_to_measure = \"\"\"\n",
        "result = sum(range(100))\n",
        "\"\"\"\n",
        "\n",
        "# Create a Timer object with the code snippet\n",
        "timer = timeit.Timer(stmt=code_to_measure)\n",
        "\n",
        "# Use autorange to determine the number of loops\n",
        "number, time_taken = timer.autorange()\n",
        "\n",
        "print(f\"Number of loops: {number}\")\n",
        "print(f\"Time taken per loop: {time_taken / number:.6f} seconds\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "0eXplImc5tau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare results:\n",
        "\n",
        "```python\n",
        "from itertools import product\n",
        "\n",
        "# Compare takes a list of measurements which we'll save in results.\n",
        "results = []\n",
        "\n",
        "sizes = [1, 64, 1024, 10000]\n",
        "for b, n in product(sizes, sizes):\n",
        "    # label and sub_label are the rows\n",
        "    # description is the column\n",
        "    label = 'Batched dot'\n",
        "    sub_label = f'[{b}, {n}]'\n",
        "    x = torch.ones((b, n))\n",
        "    for num_threads in [1, 4, 16, 32]:\n",
        "        results.append(benchmark.Timer(\n",
        "            stmt='batched_dot_mul_sum(x, x)',\n",
        "            setup='from __main__ import batched_dot_mul_sum',\n",
        "            globals={'x': x},\n",
        "            num_threads=num_threads,\n",
        "            label=label,\n",
        "            sub_label=sub_label,\n",
        "            description='mul/sum',\n",
        "        ).blocked_autorange(min_run_time=1))\n",
        "        results.append(benchmark.Timer(\n",
        "            stmt='batched_dot_bmm(x, x)',\n",
        "            setup='from __main__ import batched_dot_bmm',\n",
        "            globals={'x': x},\n",
        "            num_threads=num_threads,\n",
        "            label=label,\n",
        "            sub_label=sub_label,\n",
        "            description='bmm',\n",
        "        ).blocked_autorange(min_run_time=1))\n",
        "\n",
        "compare = benchmark.Compare(results)\n",
        "compare.print()\n",
        "```\n",
        "\n",
        "```\n",
        " [--------------- Batched dot ----------------]\n",
        "                       |  mul/sum   |    bmm\n",
        " 1 threads: -----------------------------------\n",
        "       [1, 1]          |       5.9  |      11.2\n",
        "       [1, 64]         |       6.4  |      11.4\n",
        "       [1, 1024]       |       6.7  |      14.2\n",
        "       [1, 10000]      |      10.2  |      23.7\n",
        "       [64, 1]         |       6.3  |      11.5\n",
        "       [64, 64]        |       8.6  |      15.4\n",
        "       [64, 1024]      |      39.4  |     204.4\n",
        "       [64, 10000]     |     274.9  |     748.5\n",
        "       [1024, 1]       |       7.7  |      17.8\n",
        "       [1024, 64]      |      40.3  |      76.4\n",
        "       [1024, 1024]    |     432.4  |    2795.9\n",
        "       [1024, 10000]   |   22657.3  |   11899.5\n",
        "       [10000, 1]      |      16.9  |      74.8\n",
        "       [10000, 64]     |     300.3  |     609.4\n",
        "       [10000, 1024]   |   23098.6  |   27246.1\n",
        "       [10000, 10000]  |  267073.7  |  118823.7\n",
        " 4 threads: -----------------------------------\n",
        "       [1, 1]          |       6.0  |      11.5\n",
        "       [1, 64]         |       6.2  |      11.2\n",
        "       [1, 1024]       |       6.8  |      14.3\n",
        "       [1, 10000]      |      10.2  |      23.7\n",
        "       [64, 1]         |       6.3  |      16.2\n",
        "       [64, 64]        |       8.8  |      18.2\n",
        "       [64, 1024]      |      41.5  |     189.1\n",
        "       [64, 10000]     |      91.7  |     849.1\n",
        "       [1024, 1]       |       7.6  |      17.4\n",
        "       [1024, 64]      |      43.5  |      33.5\n",
        "       [1024, 1024]    |     135.4  |    2782.3\n",
        "       [1024, 10000]   |    7471.1  |   11874.0\n",
        "       [10000, 1]      |      16.8  |      33.9\n",
        "       [10000, 64]     |     118.7  |     173.2\n",
        "       [10000, 1024]   |    7264.6  |   27824.7\n",
        "       [10000, 10000]  |  100060.9  |  121499.0\n",
        " 16 threads: ----------------------------------\n",
        "       [1, 1]          |       6.0  |      11.3\n",
        "       [1, 64]         |       6.2  |      11.2\n",
        "       [1, 1024]       |       6.9  |      14.2\n",
        "       [1, 10000]      |      10.3  |      23.8\n",
        "       [64, 1]         |       6.4  |      24.1\n",
        "       [64, 64]        |       9.0  |      23.8\n",
        "       [64, 1024]      |      54.1  |     188.5\n",
        "       [64, 10000]     |      49.9  |     748.0\n",
        "       [1024, 1]       |       7.6  |      23.4\n",
        "       [1024, 64]      |      55.5  |      28.2\n",
        "       [1024, 1024]    |      66.9  |    2773.9\n",
        "       [1024, 10000]   |    6111.5  |   12833.7\n",
        "       [10000, 1]      |      16.9  |      27.5\n",
        "       [10000, 64]     |      59.5  |      73.7\n",
        "       [10000, 1024]   |    6295.9  |   27062.0\n",
        "       [10000, 10000]  |   71804.5  |  120365.8\n",
        " 32 threads: ----------------------------------\n",
        "       [1, 1]          |       5.9  |      11.3\n",
        "       [1, 64]         |       6.2  |      11.3\n",
        "       [1, 1024]       |       6.7  |      14.2\n",
        "       [1, 10000]      |      10.5  |      23.8\n",
        "       [64, 1]         |       6.3  |      31.7\n",
        "       [64, 64]        |       9.1  |      30.4\n",
        "       [64, 1024]      |      72.0  |     190.4\n",
        "       [64, 10000]     |     103.1  |     746.9\n",
        "       [1024, 1]       |       7.6  |      28.4\n",
        "       [1024, 64]      |      70.5  |      31.9\n",
        "       [1024, 1024]    |      65.6  |    2804.6\n",
        "       [1024, 10000]   |    6764.0  |   11871.4\n",
        "       [10000, 1]      |      17.8  |      31.8\n",
        "       [10000, 64]     |     110.3  |      56.0\n",
        "       [10000, 1024]   |    6640.2  |   27592.2\n",
        "       [10000, 10000]  |   73003.4  |  120083.2\n",
        "\n",
        " Times are in microseconds (us).\n",
        "```"
      ],
      "metadata": {
        "id": "poRY_EIN7AFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profiler\n",
        "\n",
        "https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html"
      ],
      "metadata": {
        "id": "4WwFCS7nCbbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU],profile_memory=True,  with_stack=True,record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        f_t1(x)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "prof.export_chrome_trace(\"trace.json\")\n",
        "\n",
        "#### open in (chrome://tracing):"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXpjWol735fd",
        "outputId": "e601d010-91b1-4455-d763-c6a714058f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "       model_inference        55.87%       3.224ms       100.00%       5.771ms       5.771ms           0 b    -562.89 Kb             1  \n",
            "        aten::uniform_        30.81%       1.778ms        30.81%       1.778ms     444.500us           0 b           0 b             4  \n",
            "          aten::linear         0.45%      26.000us        10.78%     622.000us     311.000us     128.91 Kb           0 b             2  \n",
            "           aten::addmm         7.33%     423.000us         8.92%     515.000us     257.500us     128.91 Kb     128.91 Kb             2  \n",
            "           aten::empty         1.70%      98.000us         1.70%      98.000us      24.500us     433.98 Kb     433.98 Kb             4  \n",
            "               aten::t         0.83%      48.000us         1.40%      81.000us      40.500us           0 b           0 b             2  \n",
            "           aten::copy_         1.30%      75.000us         1.30%      75.000us      37.500us           0 b           0 b             2  \n",
            "          aten::detach         0.26%      15.000us         0.85%      49.000us      12.250us           0 b           0 b             4  \n",
            "                detach         0.59%      34.000us         0.59%      34.000us       8.500us           0 b           0 b             4  \n",
            "       aten::transpose         0.38%      22.000us         0.57%      33.000us      16.500us           0 b           0 b             2  \n",
            "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 5.771ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_threads = torch.get_num_threads()\n",
        "num_threads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHq2ImBJiSBm",
        "outputId": "f7d6f723-80f8-4f6c-b7a9-b43825814254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wA6J1SDhrBc3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}