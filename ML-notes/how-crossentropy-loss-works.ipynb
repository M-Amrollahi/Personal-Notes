{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = torch.tensor([[.0,.4,.6],[.7,0,.3]], dtype=torch.float32)\n",
    "y_p1 = y_p.softmax(dim=1)\n",
    "y = torch.tensor([[0,0,1],[1,0,0]], dtype=torch.float32)\n",
    "y1 = torch.tensor([2,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "$H(x)=-\\sum_{i}^{C} p(x)log(p(x))$\n",
    "\n",
    "$\\text{Entropy can translate as Certainty. How much we cartain about the probability of any class?}$\n",
    "\n",
    "$\\text{ If we have two classes, we may have the probability of .1 and .9. So, the certainty of picking one sample or unpicking sample is high. But, when we have the probability of .5 and .5, we can not certain about picking one or not picking it.}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How CrossEntropy calculated\n",
    "\n",
    "$loss = \\sum_{i}^{C} y.log(\\hat y)  $\n",
    "\n",
    "$\\hat y \\text{ can be eighter probability list for each class like [.2,.3,.5] or the number for that class like [2]}$\n",
    "\n",
    "$\\text{Note: CrossEntropy automomously calculate the softamx and consider it as the probability.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "torch.allclose(-1*torch.sum( y * torch.log(y_p1))/2 , criterion.forward(y_p, y1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate BinaryCrossEntropy (Log-Loss)\n",
    "\n",
    "$loss=\\dfrac{-1}{N} \\sum_{i}^{N} ylog(\\hat y) + (1-y)log(1-\\hat y)$\n",
    "\n",
    "$\\text{Note: All inputs consider as as probability, so no softmax inculdes}$\n",
    "\n",
    "$\\:$ N is numbre_of_samples * count_prob_list $\\:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = torch.tensor([[.2,.8],[.3,.7]], dtype=torch.float32)\n",
    "y = torch.tensor([[1,0],[0,1]], dtype=torch.float32)\n",
    "\n",
    "cri3 = torch.nn.BCELoss()\n",
    "torch.allclose(-1*torch.sum(y*torch.log(y_p) + (1-y)*torch.log(1-y_p))/4, cri3.forward(y_p, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = torch.tensor([[.2],[.8]], dtype=torch.float32)\n",
    "y_p1 = y_p.softmax(dim=1)\n",
    "y = torch.tensor([[0],[1]], dtype=torch.float32)\n",
    "y1 = torch.tensor([2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6612, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Sigmoid()\n",
    "loss = torch.nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate the LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cri2 = torch.nn.LogSoftmax(dim=1)   \n",
    "torch.allclose(torch.log(y_p.softmax(dim=1)) ,cri2.forward(y_p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Negative-LogLikelihood works\n",
    "\n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
    "        w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$\n",
    "\n",
    "$LogSoftmax=\n",
    "\\begin{bmatrix}\n",
    "-1.5722 & -1.2722 & -1.7722 & -1.0722 \\\\\n",
    "-1.0391 & -2.1391 & -0.8391 & -2.3391 \\\\\n",
    "-2.1627 & -1.1627 & -1.3727 & -1.1427\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$Target=\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\implies$\n",
    "\n",
    "$-1*sum(-1.2722 , -0.8391, -1.1427)/3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cri1 = torch.nn.NLLLoss()\n",
    "torch.allclose(-torch.sum(y_p*y)/2, cri1.forward(y_p,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6500)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cri1.forward(y_p,y1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10 (default, Feb 26 2021, 10:16:00) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3e886644a3928785097e4f2b76d86ea4f5782e84190f8a407df728a5fcc7bf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
